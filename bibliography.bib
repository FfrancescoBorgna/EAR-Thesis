
@online{examplewebsite,
  author        = {Wolf and Dewitt},
  note          = {2000; McGlone, 2004},
}
@online{sfm_matlab,
  note          = {Mathworks 2023},
  url           = {https://www.mathworks.com/help/vision/ug/structure-from-motion.html#:~:text=Structure%20from%20motion%20(SfM)%20is,localization%20and%20mapping%20(vSLAM).},
}
@online{open_cv,
  note          = {OpenCV 2024},
  url           = {https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html},
}
@Book{Hartley2004, 
    author = "Hartley, R.~I. and Zisserman, A.",
    title = "Multiple View Geometry in Computer Vision",
    edition = "Second",
    year = "2004",
    publisher = "Cambridge University Press, ISBN: 0521540518"
}
@online{colmap,
  note = "Colmap 2024",
  url = "https://colmap.github.io/index.html",
}
@inproceedings{schoenberger2016sfm,
    author={Sch\"{o}nberger, Johannes Lutz and Frahm, Jan-Michael},
    title={Structure-from-Motion Revisited},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016},
}

@inproceedings{schoenberger2016mvs,
    author={Sch\"{o}nberger, Johannes Lutz and Zheng, Enliang and Pollefeys, Marc and Frahm, Jan-Michael},
    title={Pixelwise View Selection for Unstructured Multi-View Stereo},
    booktitle={European Conference on Computer Vision (ECCV)},
    year={2016},
}
@ARTICLE {Ranftl2022,
    author  = "Ren\'{e} Ranftl and Katrin Lasinger and David Hafner and Konrad Schindler and Vladlen Koltun",
    title   = "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer",
    journal = "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    year    = "2022",
    volume  = "44",
    number  = "3"
}
@article{Ranftl2021,
	author    = {Ren\'{e} Ranftl and Alexey Bochkovskiy and Vladlen Koltun},
	title     = {Vision Transformers for Dense Prediction},
	journal   = {ICCV},
	year      = {2021},
}

@article{neuraldiff,
  author       = {Vadim Tschernezki and
                  Diane Larlus and
                  Andrea Vedaldi},
  title        = {NeuralDiff: Segmenting 3D objects that move in egocentric videos},
  journal      = {CoRR},
  volume       = {abs/2110.09936},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.09936},
  eprinttype    = {arXiv},
  eprint       = {2110.09936},
  timestamp    = {Mon, 25 Oct 2021 20:07:12 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-09936.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Lowe04_SIFT,
  author    = {David G. Lowe},
  title     = {Distinctive Image Features from Scale-Invariant Keypoints},
  journal   = {International Journal of Computer Vision},
  volume    = {60},
  number    = {2},
  pages     = {91--110},
  year      = {2004},
  doi       = {10.1023/B:VISI.0000029664.99615.94},
  url       = {http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf},
}
@article{BAY_SURF,
title = {Speeded-Up Robust Features (SURF)},
journal = {Computer Vision and Image Understanding},
volume = {110},
number = {3},
pages = {346-359},
year = {2008},
note = {Similarity Matching in Computer Vision and Multimedia},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2007.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S1077314207001555},
author = {Herbert Bay and Andreas Ess and Tinne Tuytelaars and Luc {Van Gool}},
keywords = {Interest points, Local features, Feature description, Camera calibration, Object recognition},
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF’s application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF’s usefulness in a broad range of topics in computer vision.}
}
@InProceedings{Brief,
author="Calonder, Michael
and Lepetit, Vincent
and Strecha, Christoph
and Fua, Pascal",
editor="Daniilidis, Kostas
and Maragos, Petros
and Paragios, Nikos",
title="BRIEF: Binary Robust Independent Elementary Features",
booktitle="Computer Vision -- ECCV 2010",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="778--792",
abstract="We propose to use binary strings as an efficient feature point descriptor, which we call BRIEF.We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to compute, instead of the L2 norm as is usually done.",
isbn="978-3-642-15561-1"
}
@INPROCEEDINGS{ORB,
  author={Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
  booktitle={2011 International Conference on Computer Vision}, 
  title={ORB: An efficient alternative to SIFT or SURF}, 
  year={2011},
  volume={},
  number={},
  pages={2564-2571},
  doi={10.1109/ICCV.2011.6126544}}

@INPROCEEDINGS{EPICKITCHENS,
   title={Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},
   author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and Fidler, Sanja and 
           Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
   booktitle={European Conference on Computer Vision (ECCV)},
   year={2018}
   }
@article{residualImage,
  author       = {Kaiming He and
                  Xiangyu Zhang and
                  Shaoqing Ren and
                  Jian Sun},
  title        = {Deep Residual Learning for Image Recognition},
  journal      = {CoRR},
  volume       = {abs/1512.03385},
  year         = {2015},
  url          = {http://arxiv.org/abs/1512.03385},
  eprinttype    = {arXiv},
  eprint       = {1512.03385},
  timestamp    = {Wed, 25 Jan 2023 11:01:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{fasterRCNN,
  author       = {Shaoqing Ren and
                  Kaiming He and
                  Ross B. Girshick and
                  Jian Sun},
  title        = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
                  Networks},
  journal      = {CoRR},
  volume       = {abs/1506.01497},
  year         = {2015},
  url          = {http://arxiv.org/abs/1506.01497},
  eprinttype    = {arXiv},
  eprint       = {1506.01497},
  timestamp    = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/RenHG015.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{captioning,
  author       = {Andrej Karpathy and
                  Li Fei{-}Fei},
  title        = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  journal      = {CoRR},
  volume       = {abs/1412.2306},
  year         = {2014},
  url          = {http://arxiv.org/abs/1412.2306},
  eprinttype    = {arXiv},
  eprint       = {1412.2306},
  timestamp    = {Wed, 15 Sep 2021 14:13:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KarpathyF14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{vqa,
author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
title = {VQA: Visual Question Answering},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}
@article{pascalImage,
  added-at = {2020-05-23T11:30:44.000+0200},
  author = {Everingham, Mark and Gool, Luc Van and Williams, Christopher K. I. and Winn, John M. and Zisserman, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/2cb5ac4f22faf09897fd86076931d682b/jan.hofmann1},
  ee = {https://www.wikidata.org/entity/Q56594395},
  interhash = {8d7846ab0aa897ffead1e7abf2dfba3e},
  intrahash = {cb5ac4f22faf09897fd86076931d682b},
  journal = {Int. J. Comput. Vis.},
  keywords = {thema:pyramid_scene_parsing},
  number = 2,
  pages = {303-338},
  timestamp = {2020-05-23T11:30:44.000+0200},
  title = {The Pascal Visual Object Classes (VOC) Challenge.},
  url = {http://dblp.uni-trier.de/db/journals/ijcv/ijcv88.html#EveringhamGWWZ10},
  volume = 88,
  year = 2010
}
@INPROCEEDINGS{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

  @article{COCO,
  author       = {Tsung{-}Yi Lin and
                  Michael Maire and
                  Serge J. Belongie and
                  Lubomir D. Bourdev and
                  Ross B. Girshick and
                  James Hays and
                  Pietro Perona and
                  Deva Ramanan and
                  Piotr Doll{\'{a}}r and
                  C. Lawrence Zitnick},
  title        = {Microsoft {COCO:} Common Objects in Context},
  journal      = {CoRR},
  volume       = {abs/1405.0312},
  year         = {2014},
  url          = {http://arxiv.org/abs/1405.0312},
  eprinttype    = {arXiv},
  eprint       = {1405.0312},
  timestamp    = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/LinMBHPRDZ14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{ADE20K,
  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Scene Parsing through ADE20K Dataset}, 
  year={2017},
  volume={},
  number={},
  pages={5122-5130},
  keywords={Image segmentation;Semantics;Sun;Labeling;Visualization;Neural networks;Computer vision},
  doi={10.1109/CVPR.2017.544}}

@article{somethingSomething,
  author       = {Raghav Goyal and
                  Samira Ebrahimi Kahou and
                  Vincent Michalski and
                  Joanna Materzynska and
                  Susanne Westphal and
                  Heuna Kim and
                  Valentin Haenel and
                  Ingo Fr{\"{u}}nd and
                  Peter Yianilos and
                  Moritz Mueller{-}Freitag and
                  Florian Hoppe and
                  Christian Thurau and
                  Ingo Bax and
                  Roland Memisevic},
  title        = {The "something something" video database for learning and evaluating
                  visual common sense},
  journal      = {CoRR},
  volume       = {abs/1706.04261},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.04261},
  eprinttype    = {arXiv},
  eprint       = {1706.04261},
  timestamp    = {Mon, 13 Aug 2018 16:48:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GoyalKMMWKHFYMH17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{yt,
  author       = {Sami Abu{-}El{-}Haija and
                  Nisarg Kothari and
                  Joonseok Lee and
                  Paul Natsev and
                  George Toderici and
                  Balakrishnan Varadarajan and
                  Sudheendra Vijayanarasimhan},
  title        = {YouTube-8M: {A} Large-Scale Video Classification Benchmark},
  journal      = {CoRR},
  volume       = {abs/1609.08675},
  year         = {2016},
  url          = {http://arxiv.org/abs/1609.08675},
  eprinttype    = {arXiv},
  eprint       = {1609.08675},
  timestamp    = {Sun, 02 Oct 2022 15:31:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/Abu-El-HaijaKLN16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{movieBench,
  author       = {Anna Rohrbach and
                  Marcus Rohrbach and
                  Niket Tandon and
                  Bernt Schiele},
  title        = {A Dataset for Movie Description},
  journal      = {CoRR},
  volume       = {abs/1501.02530},
  year         = {2015},
  url          = {http://arxiv.org/abs/1501.02530},
  eprinttype    = {arXiv},
  eprint       = {1501.02530},
  timestamp    = {Mon, 13 Aug 2018 16:47:07 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/RohrbachRTS15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{movieQA,
  author       = {Makarand Tapaswi and
                  Yukun Zhu and
                  Rainer Stiefelhagen and
                  Antonio Torralba and
                  Raquel Urtasun and
                  Sanja Fidler},
  title        = {MovieQA: Understanding Stories in Movies through Question-Answering},
  journal      = {CoRR},
  volume       = {abs/1512.02902},
  year         = {2015},
  url          = {http://arxiv.org/abs/1512.02902},
  eprinttype    = {arXiv},
  eprint       = {1512.02902},
  timestamp    = {Mon, 13 Aug 2018 16:45:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/TapaswiZSTUF15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{vlogs,
  author       = {David F. Fouhey and
                  Weicheng Kuo and
                  Alexei A. Efros and
                  Jitendra Malik},
  title        = {From Lifestyle Vlogs to Everyday Interactions},
  journal      = {CoRR},
  volume       = {abs/1712.02310},
  year         = {2017},
  url          = {http://arxiv.org/abs/1712.02310},
  eprinttype    = {arXiv},
  eprint       = {1712.02310},
  timestamp    = {Mon, 13 Aug 2018 16:48:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1712-02310.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{charades,
  author       = {Gunnar A. Sigurdsson and
                  G{\"{u}}l Varol and
                  Xiaolong Wang and
                  Ali Farhadi and
                  Ivan Laptev and
                  Abhinav Gupta},
  title        = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},
  journal      = {CoRR},
  volume       = {abs/1604.01753},
  year         = {2016},
  url          = {http://arxiv.org/abs/1604.01753},
  eprinttype    = {arXiv},
  eprint       = {1604.01753},
  timestamp    = {Fri, 05 Apr 2019 07:29:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SigurdssonVWFLG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{EK100,
           title={Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100},
           author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Furnari, Antonino 
           and Ma, Jian and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
           journal   = {International Journal of Computer Vision (IJCV)},
           year      = {2022},
           volume = {130},
           pages = {33–55},
           Url       = {https://doi.org/10.1007/s11263-021-01531-2}
} 
@misc{Word2Vec,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{visor35,
      title={Understanding Human Hands in Contact at Internet Scale}, 
      author={Dandan Shan and Jiaqi Geng and Michelle Shu and David F. Fouhey},
      year={2020},
      eprint={2006.06669},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{visor5,
      title={Large-scale interactive object segmentation with human annotators}, 
      author={Rodrigo Benenson and Stefan Popov and Vittorio Ferrari},
      year={2019},
      eprint={1903.10830},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{nerf,
      title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis}, 
      author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
      year={2020},
      eprint={2003.08934},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{neuraldiff,
      title={NeuralDiff: Segmenting 3D objects that move in egocentric videos}, 
      author={Vadim Tschernezki and Diane Larlus and Andrea Vedaldi},
      year={2021},
      eprint={2110.09936},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{nerfw,
      title={NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections}, 
      author={Ricardo Martin-Brualla and Noha Radwan and Mehdi S. M. Sajjadi and Jonathan T. Barron and Alexey Dosovitskiy and Daniel Duckworth},
      year={2021},
      eprint={2008.02268},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{Tnerf,
      title={Monocular Dynamic View Synthesis: A Reality Check}, 
      author={Hang Gao and Ruilong Li and Shubham Tulsiani and Bryan Russell and Angjoo Kanazawa},
      year={2022},
      eprint={2210.13445},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}@misc{MG,
      title={Self-supervised Video Object Segmentation by Motion Grouping}, 
      author={Charig Yang and Hala Lamdouar and Erika Lu and Andrew Zisserman and Weidi Xie},
      year={2021},
      eprint={2104.07658},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@incollection{bundle,
  author    = {Andrew W. Fitzgibbon and
               Simon W. Bowman and
               Andrew Zisserman and
               Mark A. Fischler},
  title     = {Bundle Adjustment – A Modern Synthesis},
  booktitle = {Vision Algorithms: Theory and Practice},
  publisher = {Springer-Verlag},
  year      = {2000},
  volume    = {1883},
  series    = {LNCS},
  pages     = {298--372},
}

@ARTICLE{DIML,
  author={Kim, Youngjung and Jung, Hyungjoo and Min, Dongbo and Sohn, Kwanghoon},
  journal={IEEE Transactions on Image Processing}, 
  title={Deep Monocular Depth Estimation via Integration of Global and Local Predictions}, 
  year={2018},
  volume={27},
  number={8},
  pages={4131-4144},
  keywords={Estimation;Predictive models;Databases;Training;Optimization;Measurement;Computational modeling;Depth estimation;2D-to-3D conversion;non-parametric sampling;convolutional neural networks;RGB-D database},
  doi={10.1109/TIP.2018.2836318}}
@misc{megadepth,
      title={MegaDepth: Learning Single-View Depth Prediction from Internet Photos}, 
      author={Zhengqi Li and Noah Snavely},
      year={2018},
      eprint={1804.00607},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@INPROCEEDINGS{redweb,
  author={Xian, Ke and Shen, Chunhua and Cao, Zhiguo and Lu, Hao and Xiao, Yang and Li, Ruibo and Luo, Zhenbo},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Monocular Relative Depth Perception with Web Stereo Data Supervision}, 
  year={2018},
  volume={},
  number={},
  pages={311-320},
  keywords={Training;Measurement;Task analysis;Semantics;Estimation;Image segmentation;Network architecture},
  doi={10.1109/CVPR.2018.00040}}
@misc{wsvd,
      title={Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes}, 
      author={Chaoyang Wang and Simon Lucey and Federico Perazzi and Oliver Wang},
      year={2019},
      eprint={1904.11112},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{diw,
      title={Single-Image Depth Perception in the Wild}, 
      author={Weifeng Chen and Zhao Fu and Dawei Yang and Jia Deng},
      year={2017},
      eprint={1604.03901},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@INPROCEEDINGS{eth3d,
  author={Schöps, Thomas and Schönberger, Johannes L. and Galliani, Silvano and Sattler, Torsten and Schindler, Konrad and Pollefeys, Marc and Geiger, Andreas},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Multi-view Stereo Benchmark with High-Resolution Images and Multi-camera Videos}, 
  year={2017},
  volume={},
  number={},
  pages={2538-2547},
  keywords={Benchmark testing;Cameras;Three-dimensional displays;Lasers;Image resolution;Videos;Robot vision systems},
  doi={10.1109/CVPR.2017.272}}
@InProceedings{sintel,
author="Butler, Daniel J.
and Wulff, Jonas
and Stanley, Garrett B.
and Black, Michael J.",
editor="Fitzgibbon, Andrew
and Lazebnik, Svetlana
and Perona, Pietro
and Sato, Yoichi
and Schmid, Cordelia",
title="A Naturalistic Open Source Movie for Optical Flow Evaluation",
booktitle="Computer Vision -- ECCV 2012",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="611--625",
abstract="Ground truth optical flow is difficult to measure in real scenes with natural motion. As a result, optical flow data sets are restricted in terms of size, complexity, and diversity, making optical flow algorithms difficult to train and test on realistic data. We introduce a new optical flow data set derived from the open source 3D animated short film Sintel. This data set has important features not present in the popular Middlebury flow evaluation: long sequences, large motions, specular reflections, motion blur, defocus blur, and atmospheric effects. Because the graphics data that generated the movie is open source, we are able to render scenes under conditions of varying complexity to evaluate where existing flow algorithms fail. We evaluate several recent optical flow algorithms and find that current highly-ranked methods on the Middlebury evaluation have difficulty with this more complex data set suggesting further research on optical flow estimation is needed. To validate the use of synthetic data, we compare the image- and flow-statistics of Sintel to those of real films and videos and show that they are similar. The data set, metrics, and evaluation website are publicly available.",
isbn="978-3-642-33783-3"
}

@INPROCEEDINGS{kitti,
  author={Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Are we ready for autonomous driving? The KITTI vision benchmark suite}, 
  year={2012},
  volume={},
  number={},
  pages={3354-3361},
  keywords={Benchmark testing;Cameras;Optical imaging;Visualization;Optical sensors;Measurement},
  doi={10.1109/CVPR.2012.6248074}}
@InProceedings{nyudv2,
author="Silberman, Nathan
and Hoiem, Derek
and Kohli, Pushmeet
and Fergus, Rob",
editor="Fitzgibbon, Andrew
and Lazebnik, Svetlana
and Perona, Pietro
and Sato, Yoichi
and Schmid, Cordelia",
title="Indoor Segmentation and Support Inference from RGBD Images",
booktitle="Computer Vision -- ECCV 2012",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="746--760",
abstract="We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.",
isbn="978-3-642-33715-4"
}
@INPROCEEDINGS{tum_rgbd,
  author={Sturm, Jürgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={A benchmark for the evaluation of RGB-D SLAM systems}, 
  year={2012},
  volume={},
  number={},
  pages={573-580},
  keywords={Cameras;Simultaneous localization and mapping;Calibration;Trajectory;Visualization},
  doi={10.1109/IROS.2012.6385773}}

