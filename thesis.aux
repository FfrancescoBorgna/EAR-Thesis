\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand*\new@tpo@label[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{none/global//global/global}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{abbreviations}{glg-abr}{gls-abr}{glo-abr}
\providecommand\@glsxtr@savepreloctag[2]{}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\babel@aux{italian}{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{vi}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vii}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Acronyms}{x}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Related Works}{1}{part.1}\protected@file@percent }
\newlabel{sec:Related}{{I}{2}{Related Works}{part.1}{}}
\abx@aux@cite{0}{EPICKITCHENS}
\abx@aux@segm{0}{0}{EPICKITCHENS}
\abx@aux@cite{0}{residualImage}
\abx@aux@segm{0}{0}{residualImage}
\abx@aux@cite{0}{fasterRCNN}
\abx@aux@segm{0}{0}{fasterRCNN}
\abx@aux@cite{0}{captioning}
\abx@aux@segm{0}{0}{captioning}
\abx@aux@cite{0}{vqa}
\abx@aux@segm{0}{0}{vqa}
\abx@aux@cite{0}{pascalImage}
\abx@aux@segm{0}{0}{pascalImage}
\abx@aux@cite{0}{imagenet}
\abx@aux@segm{0}{0}{imagenet}
\abx@aux@cite{0}{COCO}
\abx@aux@segm{0}{0}{COCO}
\abx@aux@cite{0}{ADE20K}
\abx@aux@segm{0}{0}{ADE20K}
\abx@aux@cite{0}{somethingSomething}
\abx@aux@segm{0}{0}{somethingSomething}
\abx@aux@cite{0}{yt}
\abx@aux@segm{0}{0}{yt}
\abx@aux@cite{0}{movieBench}
\abx@aux@segm{0}{0}{movieBench}
\abx@aux@cite{0}{movieQA}
\abx@aux@segm{0}{0}{movieQA}
\abx@aux@cite{0}{vlogs}
\abx@aux@segm{0}{0}{vlogs}
\abx@aux@cite{0}{movieQA}
\abx@aux@segm{0}{0}{movieQA}
\abx@aux@cite{0}{charades}
\abx@aux@segm{0}{0}{charades}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Datasets}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}EPIC-Kitchens}{3}{section.1.1}\protected@file@percent }
\newlabel{sec:EK}{{1.1}{3}{EPIC-Kitchens}{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Introduction/motivation}{3}{subsection.1.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Comparative overview of relevant datasets(action classes with > 50 samples)}}{4}{table.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:epic_comparison}{{1.1}{4}{Comparative overview of relevant datasets(action classes with > 50 samples)}{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Data Collection}{4}{subsection.1.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf  {Top} (left to right): time of day of the recording, pie chart of high-level goals, histogram of sequence durations and dataset logo; \textbf  {Bottom}:Wordles of narrations in native languages(English, Italian, Spanish, Greek and Chinese).}}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:epic_stat}{{1.1}{5}{\textbf {Top} (left to right): time of day of the recording, pie chart of high-level goals, histogram of sequence durations and dataset logo; \textbf {Bottom}:Wordles of narrations in native languages(English, Italian, Spanish, Greek and Chinese)}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Data Annotation pipeline}{5}{subsection.1.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Narration Guidelines given to each participant to be followed after the completion of a recording.}}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:epic_guidelines}{{1.2}{6}{Narration Guidelines given to each participant to be followed after the completion of a recording}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Extracts from 6 transcription files in .sbv format}}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig:epic_trans}{{1.3}{6}{Extracts from 6 transcription files in .sbv format}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Example of annotated action segments for 2 consecutive actions}}{7}{figure.caption.8}\protected@file@percent }
\newlabel{fig:epic_anno}{{1.4}{7}{Example of annotated action segments for 2 consecutive actions}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Sample Verb and Noun Classes}}{7}{figure.caption.9}\protected@file@percent }
\newlabel{fig:epic_table}{{1.5}{7}{Sample Verb and Noun Classes}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces \textbf  {From Top:} Frequency of verb classes in action segments; Frequency of noun clusters in action segments, by category; Frequency of noun clusters in bounding box annotations, by category; Mean and standard deviation of bounding box, by category}}{8}{figure.caption.10}\protected@file@percent }
\newlabel{fig:epic_freq}{{1.6}{8}{\textbf {From Top:} Frequency of verb classes in action segments; Frequency of noun clusters in action segments, by category; Frequency of noun clusters in bounding box annotations, by category; Mean and standard deviation of bounding box, by category}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Sample consecutive action segments with keyframe object annotations}}{9}{figure.caption.11}\protected@file@percent }
\newlabel{fig:epic_bb}{{1.7}{9}{Sample consecutive action segments with keyframe object annotations}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Benchmarks and Baseline Results}{9}{subsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Action Recognition Challenge}{9}{section*.12}\protected@file@percent }
\newlabel{sec:ep_AR_chall}{{1.1.4}{9}{Action Recognition Challenge}{section*.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Sample qualitative results from the challenge's baseline of the Action Recognition Task}}{10}{figure.caption.13}\protected@file@percent }
\newlabel{fig:epic_ar_chall}{{1.8}{10}{Sample qualitative results from the challenge's baseline of the Action Recognition Task}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Action Anticipation Challenge}{10}{section*.14}\protected@file@percent }
\newlabel{sec:ep_AA_chall}{{1.1.4}{10}{Action Anticipation Challenge}{section*.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Sample qualitative results from the challenge's baseline of the Action Anticipation Task}}{10}{figure.caption.15}\protected@file@percent }
\newlabel{fig:epic_aa_chall}{{1.9}{10}{Sample qualitative results from the challenge's baseline of the Action Anticipation Task}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{Object Detection Challenge}{10}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Dataset Release}{10}{subsection.1.1.5}\protected@file@percent }
\abx@aux@cite{0}{EK100}
\abx@aux@segm{0}{0}{EK100}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Sample qualitative results from the challenge's baseline of the Object Detection Task}}{11}{figure.caption.17}\protected@file@percent }
\newlabel{fig:epic_od_chall}{{1.10}{11}{Sample qualitative results from the challenge's baseline of the Object Detection Task}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}EPIC-Kitchens 100}{11}{section.1.2}\protected@file@percent }
\newlabel{sec:EK100}{{1.2}{11}{EPIC-Kitchens 100}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Motivation}{11}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Data Collection}{12}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Annotation}{12}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Narrator}{12}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Transcriber}{12}{section*.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Annotation pipeline: \textbf  {a} narrator, \textbf  {b} transcriber \textbf  {c} temporal segment annotator and \textbf  {d} dependency parser. Red arrows show AMT crowdsourcing of annotations.}}{13}{figure.caption.18}\protected@file@percent }
\newlabel{fig:e100_ann}{{1.11}{13}{Annotation pipeline: \textbf {a} narrator, \textbf {b} transcriber \textbf {c} temporal segment annotator and \textbf {d} dependency parser. Red arrows show AMT crowdsourcing of annotations}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Parser}{13}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Temporal Annotator}{13}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Quality Improvements}{14}{subsection.1.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Comparing non-stop narrations (blue) to 'pause-and-talk' narrations (red). Right: timestsamps (dots) and segments (bars) for two sample sequences. "pause-and-talk" captures all actions including short ones. Black frames depict missed actions.}}{14}{figure.caption.23}\protected@file@percent }
\newlabel{fig:ep100_comp}{{1.12}{14}{Comparing non-stop narrations (blue) to 'pause-and-talk' narrations (red). Right: timestsamps (dots) and segments (bars) for two sample sequences. "pause-and-talk" captures all actions including short ones. Black frames depict missed actions}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Challenges and Baselines}{14}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}EPIC-Fields}{14}{section.1.3}\protected@file@percent }
\abx@aux@cite{0}{visor35}
\abx@aux@segm{0}{0}{visor35}
\abx@aux@cite{0}{visor5}
\abx@aux@segm{0}{0}{visor5}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Data}{15}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation.}{15}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Collection}{15}{section*.25}\protected@file@percent }
\abx@aux@cite{0}{nerf}
\abx@aux@segm{0}{0}{nerf}
\@writefile{toc}{\contentsline {subsubsection}{Filtering}{16}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Sparse reconstruction}{16}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dense reconstruction, automated verification, and restart}{16}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Benchmarks, Experiments and Results}{16}{subsection.1.3.2}\protected@file@percent }
\abx@aux@cite{0}{neuraldiff}
\abx@aux@segm{0}{0}{neuraldiff}
\abx@aux@cite{0}{nerfw}
\abx@aux@segm{0}{0}{nerfw}
\abx@aux@cite{0}{Tnerf}
\abx@aux@segm{0}{0}{Tnerf}
\abx@aux@cite{0}{nerfw}
\abx@aux@segm{0}{0}{nerfw}
\abx@aux@cite{0}{Tnerf}
\abx@aux@segm{0}{0}{Tnerf}
\abx@aux@cite{0}{neuraldiff}
\abx@aux@segm{0}{0}{neuraldiff}
\abx@aux@cite{0}{neuraldiff}
\abx@aux@segm{0}{0}{neuraldiff}
\@writefile{toc}{\contentsline {subsubsection}{New-View Synthesis(NVS)}{17}{section*.29}\protected@file@percent }
\abx@aux@backref{1}{nerf}{0}{17}{17}
\abx@aux@backref{2}{neuraldiff}{0}{17}{17}
\abx@aux@cite{0}{MG}
\abx@aux@segm{0}{0}{MG}
\abx@aux@cite{0}{MG}
\abx@aux@segm{0}{0}{MG}
\abx@aux@cite{0}{nerfw}
\abx@aux@segm{0}{0}{nerfw}
\abx@aux@cite{0}{Tnerf}
\abx@aux@segm{0}{0}{Tnerf}
\abx@aux@cite{0}{neuraldiff}
\abx@aux@segm{0}{0}{neuraldiff}
\abx@aux@backref{3}{neuraldiff}{0}{18}{18}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces \textbf  {Dynamic New View Synthesis.}Comparison of different neural rendering methods on varying difficult frames. The values reported corresponds to PSNR considering all pixels in each test frame.}}{18}{table.caption.30}\protected@file@percent }
\newlabel{tab:NVS_comp}{{1.2}{18}{\textbf {Dynamic New View Synthesis.}Comparison of different neural rendering methods on varying difficult frames. The values reported corresponds to PSNR considering all pixels in each test frame}{table.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Unsupervised Dynamic Object Segmentation(UDOS)}{18}{section*.32}\protected@file@percent }
\abx@aux@backref{4}{neuraldiff}{0}{18}{18}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces \textbf  {Dynamic New View Synthesis}. I report an example of the output for the three different methods used: NeRF-W, T-NeRF+ and NeuralDiff. We can see how the initial labelling of difficulty for frames was actually accurate as the reconstructions struggle with Hard frames.}}{19}{figure.caption.31}\protected@file@percent }
\newlabel{fig:NVS}{{1.13}{19}{\textbf {Dynamic New View Synthesis}. I report an example of the output for the three different methods used: NeRF-W, T-NeRF+ and NeuralDiff. We can see how the initial labelling of difficulty for frames was actually accurate as the reconstructions struggle with Hard frames}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{Semi-Supervised Video Object Segmentation(VOS)}{19}{section*.35}\protected@file@percent }
\abx@aux@cite{0}{visor}
\abx@aux@segm{0}{0}{visor}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces \textbf  {UDOS.} Here is reported the comparison of the different methods' output. The 2D based perform very good on dynamic objects, while 3D methods struggle a bit but can detect even semi-static objects.}}{20}{figure.caption.33}\protected@file@percent }
\newlabel{fig:UDOS}{{1.14}{20}{\textbf {UDOS.} Here is reported the comparison of the different methods' output. The 2D based perform very good on dynamic objects, while 3D methods struggle a bit but can detect even semi-static objects}{figure.caption.33}{}}
\abx@aux@backref{5}{neuraldiff}{0}{20}{20}
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces \textbf  {UDOS.} Here I reported the author results for UDOS. The values visible corresponds to the mAP on segmenting semi-static(SS) and dynamic(Dyn) components of the scene.}}{20}{table.caption.34}\protected@file@percent }
\newlabel{tab:UDOS}{{1.3}{20}{\textbf {UDOS.} Here I reported the author results for UDOS. The values visible corresponds to the mAP on segmenting semi-static(SS) and dynamic(Dyn) components of the scene}{table.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}VISOR}{20}{section.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces \textbf  {VOS.} Here is reported the comparison of the two different methods. The 3D method is clearly beter having as output something really close to the groundtruth. The 2D method instead is performing poorly.}}{21}{figure.caption.36}\protected@file@percent }
\newlabel{fig:VOS}{{1.15}{21}{\textbf {VOS.} Here is reported the comparison of the two different methods. The 3D method is clearly beter having as output something really close to the groundtruth. The 2D method instead is performing poorly}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces \textbf  {VISOR Annotations and Benchmarks.}Sparse annotations of the P06-03 scene, where flour becomes dough in the end.Each color represents a different object. In the bottom part the three proposed challenges are reported for the current scene. Namely: Semi-Supervised Video Object Semgentation(VOS), Hand Object Segmentation(HOS) and Where Did This Come From(WDTCF).}}{21}{figure.caption.37}\protected@file@percent }
\newlabel{fig:visor}{{1.16}{21}{\textbf {VISOR Annotations and Benchmarks.}Sparse annotations of the P06-03 scene, where flour becomes dough in the end.Each color represents a different object. In the bottom part the three proposed challenges are reported for the current scene. Namely: Semi-Supervised Video Object Semgentation(VOS), Hand Object Segmentation(HOS) and Where Did This Come From(WDTCF)}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Other Egocentric Datasets}{22}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Ego4D}{22}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Da Sistemare}{23}{part.2}\protected@file@percent }
\abx@aux@cite{0}{neuraldiff}
\abx@aux@segm{0}{0}{neuraldiff}
\newlabel{sec:Sistemare}{{II}{24}{Da Sistemare}{part.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Metrics}{24}{section.1.6}\protected@file@percent }
\newlabel{sec:Metrics}{{1.6}{24}{Metrics}{section.1.6}{}}
\abx@aux@backref{6}{neuraldiff}{0}{24}{24}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}PSNR:Peak signal-to-noise ratio}{24}{subsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}AP:Average Precision}{24}{subsection.1.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Example of Precision-recall curve.We can see how the bottom line model represents the worst a model can perform, e.g. predict every sample as it is coming from the same class,if the dataset is balanced. A better model would \textit  {tend} to the upper-right corner, which instead represents the best possible model, a model that have maximum precision and recall.}}{25}{figure.caption.38}\protected@file@percent }
\newlabel{fig:auc}{{1.17}{25}{Example of Precision-recall curve.We can see how the bottom line model represents the worst a model can perform, e.g. predict every sample as it is coming from the same class,if the dataset is balanced. A better model would \textit {tend} to the upper-right corner, which instead represents the best possible model, a model that have maximum precision and recall}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Sampling}{25}{section.1.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.4}{\ignorespaces COLMAP execution times for each scene.}}{26}{table.caption.39}\protected@file@percent }
\newlabel{tab:colmap_times}{{1.4}{26}{COLMAP execution times for each scene}{table.caption.39}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Galileo}{27}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:appendix_galileo}{{A}{27}{Sampling}{appendix.A}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Math Notation}{28}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:MathNotation}{{B}{28}{Sampling}{appendix.B}{}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{29}{appendix.B}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{EF9374385FB91EDF2A4AC8D5983A60A4}
\abx@aux@read@bblrerun
\abx@aux@defaultrefcontext{0}{CVPRtutorial}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf15}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf32}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf11}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf27}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf29}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf42}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf21}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf5}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf48}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf2}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf4}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf10}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf23}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf19}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf9}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf13}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf17}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf16}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf26}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf35}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{deepvoxels}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{LLFF}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{neuralvol}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{srn}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{neuraldiff}{none/global//global/global}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{42}
