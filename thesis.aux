\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand*\new@tpo@label[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{none/global//global/global}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{abbreviations}{glg-abr}{gls-abr}{glo-abr}
\providecommand\@glsxtr@savepreloctag[2]{}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\babel@aux{italian}{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{vi}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vii}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Acronyms}{x}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Related Works}{1}{part.1}\protected@file@percent }
\newlabel{sec:Related}{{I}{2}{Related Works}{part.1}{}}
\abx@aux@cite{0}{CVPRtutorial}
\abx@aux@segm{0}{0}{CVPRtutorial}
\abx@aux@cite{0}{nerf}
\abx@aux@segm{0}{0}{nerf}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Neural Rendering}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:Neural}{{1}{3}{Related Works}{chapter.1}{}}
\abx@aux@backref{1}{CVPRtutorial}{0}{3}{3}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}NeRF:Representing Scenes as Neural Radiance Fields for View Synthesis}{3}{section.1.1}\protected@file@percent }
\newlabel{sec:nerf}{{1.1}{3}{NeRF:Representing Scenes as Neural Radiance Fields for View Synthesis}{section.1.1}{}}
\abx@aux@backref{2}{nerf}{0}{3}{3}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Intro?}{3}{subsection.1.1.1}\protected@file@percent }
\abx@aux@cite{0}{nerf15}
\abx@aux@segm{0}{0}{nerf15}
\abx@aux@cite{0}{nerf32}
\abx@aux@segm{0}{0}{nerf32}
\abx@aux@cite{0}{nerf11}
\abx@aux@segm{0}{0}{nerf11}
\abx@aux@cite{0}{nerf27}
\abx@aux@segm{0}{0}{nerf27}
\abx@aux@cite{0}{nerf29}
\abx@aux@segm{0}{0}{nerf29}
\abx@aux@cite{0}{nerf42}
\abx@aux@segm{0}{0}{nerf42}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf  {NeRF.}Optimization of a continous 5D neural radiance field representation(volume density and view-dependent color at any continuous location) of a scene from a seet of input images. The 2D novel views are obtained thanks to classic volume rendering techniques. Here in this example, given 100 images acquired from different viewpoints, they sample two novel views.}}{4}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:batteria}{{1.1}{4}{\textbf {NeRF.}Optimization of a continous 5D neural radiance field representation(volume density and view-dependent color at any continuous location) of a scene from a seet of input images. The 2D novel views are obtained thanks to classic volume rendering techniques. Here in this example, given 100 images acquired from different viewpoints, they sample two novel views}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Related works}{4}{subsection.1.1.2}\protected@file@percent }
\abx@aux@backref{3}{nerf15}{0}{4}{4}
\abx@aux@backref{4}{nerf32}{0}{4}{4}
\abx@aux@backref{5}{nerf11}{0}{4}{4}
\abx@aux@backref{6}{nerf27}{0}{4}{4}
\abx@aux@cite{0}{nerf21}
\abx@aux@segm{0}{0}{nerf21}
\abx@aux@cite{0}{nerf5}
\abx@aux@segm{0}{0}{nerf5}
\abx@aux@cite{0}{nerf48}
\abx@aux@segm{0}{0}{nerf48}
\abx@aux@cite{0}{nerf2}
\abx@aux@segm{0}{0}{nerf2}
\abx@aux@cite{0}{nerf4}
\abx@aux@segm{0}{0}{nerf4}
\abx@aux@cite{0}{nerf10}
\abx@aux@segm{0}{0}{nerf10}
\abx@aux@cite{0}{nerf23}
\abx@aux@segm{0}{0}{nerf23}
\abx@aux@cite{0}{nerf19}
\abx@aux@segm{0}{0}{nerf19}
\abx@aux@cite{0}{nerf9}
\abx@aux@segm{0}{0}{nerf9}
\abx@aux@cite{0}{nerf13}
\abx@aux@segm{0}{0}{nerf13}
\abx@aux@cite{0}{nerf17}
\abx@aux@segm{0}{0}{nerf17}
\abx@aux@backref{7}{nerf29}{0}{5}{5}
\abx@aux@backref{8}{nerf42}{0}{5}{5}
\abx@aux@backref{9}{nerf21}{0}{5}{5}
\abx@aux@backref{10}{nerf5}{0}{5}{5}
\abx@aux@backref{11}{nerf48}{0}{5}{5}
\abx@aux@backref{12}{nerf2}{0}{5}{5}
\abx@aux@backref{13}{nerf4}{0}{5}{5}
\abx@aux@backref{14}{nerf10}{0}{5}{5}
\abx@aux@backref{15}{nerf23}{0}{5}{5}
\abx@aux@backref{16}{nerf19}{0}{5}{5}
\abx@aux@backref{17}{nerf9}{0}{5}{5}
\abx@aux@backref{18}{nerf13}{0}{5}{5}
\abx@aux@backref{19}{nerf17}{0}{5}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Implementation}{5}{subsection.1.1.3}\protected@file@percent }
\abx@aux@cite{0}{nerf16}
\abx@aux@segm{0}{0}{nerf16}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf  {$F_{\Theta }$ Scheme.} The input position \textbf  {x} pass through 8 Fully connected (FC) layers of 256-channels. Each FC layer is followed by a ReLU activation function. This intermediate result is then concatenated with the input direction (\textbf  {d}) and fed to one last FC with 128 channels that feeds its output to a ReLU function. The output of the ReLU are the color \textbf  {c} and the volume density ($\sigma $).}}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:mlp}{{1.2}{6}{\textbf {$F_{\Theta }$ Scheme.} The input position \textbf {x} pass through 8 Fully connected (FC) layers of 256-channels. Each FC layer is followed by a ReLU activation function. This intermediate result is then concatenated with the input direction (\textbf {d}) and fed to one last FC with 128 channels that feeds its output to a ReLU function. The output of the ReLU are the color \textbf {c} and the volume density ($\sigma $)}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Here are reported the results obtained with different strategies, as written underneath each image. In particular removing view dependence prevents the model from recreating the specular reflection on the bulldozer tread. Removing the Positional encoding instead we obtain a blurred image, meaning that high frequencies are not captured nor represented.}}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:lego}{{1.3}{6}{Here are reported the results obtained with different strategies, as written underneath each image. In particular removing view dependence prevents the model from recreating the specular reflection on the bulldozer tread. Removing the Positional encoding instead we obtain a blurred image, meaning that high frequencies are not captured nor represented}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Volume Rendering with Radiance Fields}{6}{section*.7}\protected@file@percent }
\abx@aux@cite{0}{nerf26}
\abx@aux@segm{0}{0}{nerf26}
\abx@aux@backref{20}{nerf16}{0}{7}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Example of rays passing through an image plane of size 3x3 pixels.}}{7}{figure.caption.8}\protected@file@percent }
\newlabel{fig:rt}{{1.4}{7}{Example of rays passing through an image plane of size 3x3 pixels}{figure.caption.8}{}}
\abx@aux@backref{21}{nerf26}{0}{7}{7}
\newlabel{eq:neural_C}{{1.4}{7}{Volume Rendering with Radiance Fields}{equation.1.1.4}{}}
\abx@aux@cite{0}{nerf35}
\abx@aux@segm{0}{0}{nerf35}
\@writefile{toc}{\contentsline {subsubsection}{Optimizing a Neural Radiance Field}{8}{section*.9}\protected@file@percent }
\abx@aux@backref{22}{nerf35}{0}{8}{8}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces PDF of normalized coarse weights $\hat  {w}_i$ along a ray with $N_c$ samples.}}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig:pdf_ray}{{1.5}{9}{PDF of normalized coarse weights $\hat {w}_i$ along a ray with $N_c$ samples}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Actual Implementation}{9}{section*.11}\protected@file@percent }
\abx@aux@cite{0}{deepvoxels}
\abx@aux@segm{0}{0}{deepvoxels}
\abx@aux@cite{0}{lambertian}
\abx@aux@segm{0}{0}{lambertian}
\abx@aux@cite{0}{LLFF}
\abx@aux@segm{0}{0}{LLFF}
\abx@aux@cite{0}{neuralvol}
\abx@aux@segm{0}{0}{neuralvol}
\abx@aux@cite{0}{srn}
\abx@aux@segm{0}{0}{srn}
\abx@aux@cite{0}{LLFF}
\abx@aux@segm{0}{0}{LLFF}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Results}{10}{subsection.1.1.4}\protected@file@percent }
\abx@aux@backref{23}{deepvoxels}{0}{10}{10}
\abx@aux@backref{24}{LLFF}{0}{10}{10}
\abx@aux@backref{25}{neuralvol}{0}{10}{10}
\abx@aux@backref{26}{srn}{0}{10}{10}
\abx@aux@backref{27}{LLFF}{0}{11}{11}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces \textbf  {Quantitative results} In all datasets, for all metrics,except for the LPIPS, the NeRF method outperforms old methods. }}{11}{table.caption.12}\protected@file@percent }
\newlabel{tab:nerf_res}{{1.1}{11}{\textbf {Quantitative results} In all datasets, for all metrics,except for the LPIPS, the NeRF method outperforms old methods}{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces \textbf  {Quantitative results} In all datasets, for all metrics,except for the LPIPS, the NeRF method outperforms old methods. }}{11}{table.caption.15}\protected@file@percent }
\newlabel{tab:nerf_ablation}{{1.2}{11}{\textbf {Quantitative results} In all datasets, for all metrics,except for the LPIPS, the NeRF method outperforms old methods}{table.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}NeuralDiff: Segmenting 3D objects that move in egocentric videos}{11}{section.1.2}\protected@file@percent }
\abx@aux@cite{0}{nerf}
\abx@aux@segm{0}{0}{nerf}
\abx@aux@backref{28}{nerf}{0}{12}{12}
\abx@aux@cite{0}{EPICKITCHENS}
\abx@aux@segm{0}{0}{EPICKITCHENS}
\abx@aux@cite{0}{ndiff_2}
\abx@aux@segm{0}{0}{ndiff_2}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Given an egocentric video with camera reconstruction, NeuralDiff, a neural architecture, learns how to decompose each frame into a static background and a a dynamic foreground, which includes every object that sooner or later will move and the actor's body parts. Each of these streams is learned exploiting the characteristics of the scene that is going to be captured. Being a neural radiance field, NeuralDiff is also capable to render images from novel viewpoints as can be seen in the bottom right part of the scene.}}{13}{figure.caption.16}\protected@file@percent }
\newlabel{fig:ndiff_1}{{1.8}{13}{Given an egocentric video with camera reconstruction, NeuralDiff, a neural architecture, learns how to decompose each frame into a static background and a a dynamic foreground, which includes every object that sooner or later will move and the actor's body parts. Each of these streams is learned exploiting the characteristics of the scene that is going to be captured. Being a neural radiance field, NeuralDiff is also capable to render images from novel viewpoints as can be seen in the bottom right part of the scene}{figure.caption.16}{}}
\abx@aux@backref{29}{EPICKITCHENS}{0}{13}{13}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Related Work}{13}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Background Subtraction}{13}{section*.17}\protected@file@percent }
\abx@aux@backref{30}{ndiff_2}{0}{13}{13}
\abx@aux@cite{0}{ndiff_18}
\abx@aux@segm{0}{0}{ndiff_18}
\abx@aux@cite{0}{ndiff_1}
\abx@aux@segm{0}{0}{ndiff_1}
\abx@aux@cite{0}{ndiff_3}
\abx@aux@segm{0}{0}{ndiff_3}
\abx@aux@cite{0}{ndiff_38}
\abx@aux@segm{0}{0}{ndiff_38}
\abx@aux@cite{0}{nerf}
\abx@aux@segm{0}{0}{nerf}
\abx@aux@cite{0}{ndiff_17}
\abx@aux@segm{0}{0}{ndiff_17}
\abx@aux@cite{0}{ndiff_30}
\abx@aux@segm{0}{0}{ndiff_30}
\abx@aux@cite{0}{ndiff_15}
\abx@aux@segm{0}{0}{ndiff_15}
\abx@aux@cite{0}{ndiff_34}
\abx@aux@segm{0}{0}{ndiff_34}
\abx@aux@cite{0}{ndiff_34}
\abx@aux@segm{0}{0}{ndiff_34}
\abx@aux@cite{0}{ndiff_15}
\abx@aux@segm{0}{0}{ndiff_15}
\@writefile{toc}{\contentsline {subsubsection}{Motion segmentation}{14}{section*.18}\protected@file@percent }
\abx@aux@backref{31}{ndiff_18}{0}{14}{14}
\@writefile{toc}{\contentsline {subsubsection}{Discovering and segmenting objects in videos}{14}{section*.19}\protected@file@percent }
\abx@aux@backref{32}{ndiff_1}{0}{14}{14}
\abx@aux@backref{33}{ndiff_3}{0}{14}{14}
\abx@aux@backref{34}{ndiff_38}{0}{14}{14}
\@writefile{toc}{\contentsline {subsubsection}{Neural rendering}{14}{section*.20}\protected@file@percent }
\abx@aux@backref{35}{nerf}{0}{14}{14}
\abx@aux@backref{36}{ndiff_17}{0}{14}{14}
\abx@aux@backref{37}{ndiff_30}{0}{14}{14}
\abx@aux@backref{38}{ndiff_15}{0}{14}{14}
\abx@aux@backref{39}{ndiff_34}{0}{14}{14}
\abx@aux@backref{40}{ndiff_34}{0}{14}{14}
\abx@aux@cite{0}{colmap}
\abx@aux@segm{0}{0}{colmap}
\abx@aux@backref{41}{ndiff_15}{0}{15}{15}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Implementation}{15}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Neural rendering}{15}{section*.21}\protected@file@percent }
\abx@aux@backref{42}{colmap}{0}{15}{15}
\abx@aux@cite{0}{nerfw}
\abx@aux@segm{0}{0}{nerfw}
\@writefile{toc}{\contentsline {subsubsection}{Dynamic Components}{16}{section*.22}\protected@file@percent }
\newlabel{eq:2}{{1.9}{16}{Dynamic Components}{equation.1.2.9}{}}
\abx@aux@backref{43}{nerfw}{0}{16}{16}
\newlabel{eq:weights}{{1.10}{16}{Dynamic Components}{equation.1.2.10}{}}
\newlabel{eq:timeNdiff}{{1.11}{16}{Dynamic Components}{equation.1.2.11}{}}
\abx@aux@cite{0}{ndiff_17}
\abx@aux@segm{0}{0}{ndiff_17}
\@writefile{toc}{\contentsline {subsubsection}{Uncertainty and regularization}{17}{section*.23}\protected@file@percent }
\abx@aux@backref{44}{ndiff_17}{0}{17}{17}
\abx@aux@cite{0}{EPICKITCHENS}
\abx@aux@segm{0}{0}{EPICKITCHENS}
\abx@aux@cite{0}{colmap}
\abx@aux@segm{0}{0}{colmap}
\abx@aux@cite{0}{nerf}
\abx@aux@segm{0}{0}{nerf}
\abx@aux@cite{0}{ndiff_17}
\abx@aux@segm{0}{0}{ndiff_17}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}EPIC-Diff benchmark}{18}{subsection.1.2.3}\protected@file@percent }
\abx@aux@backref{45}{EPICKITCHENS}{0}{18}{18}
\abx@aux@backref{46}{colmap}{0}{18}{18}
\abx@aux@backref{47}{nerf}{0}{18}{18}
\abx@aux@cite{0}{nerf}
\abx@aux@segm{0}{0}{nerf}
\abx@aux@cite{0}{nerfw}
\abx@aux@segm{0}{0}{nerfw}
\abx@aux@backref{48}{ndiff_17}{0}{19}{19}
\abx@aux@backref{49}{nerf}{0}{19}{19}
\abx@aux@backref{50}{nerfw}{0}{19}{19}
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces Results}}{19}{table.caption.25}\protected@file@percent }
\newlabel{tab:ndiff_res}{{1.3}{19}{Results}{table.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Comparison on test images from the newly introduced synthetic dataset. NeRF method is able to recover fine details in both geometry and appearance. LLFF exhibits some artifacts on the microphone and some ghosting artifact in the other scenes. SRN produces distorted and blurry rendering for every scene. Neural Volumesstruggle capturing details we can see from the ship reeconstruction.}}{20}{figure.caption.13}\protected@file@percent }
\newlabel{fig:blender2}{{1.6}{20}{Comparison on test images from the newly introduced synthetic dataset. NeRF method is able to recover fine details in both geometry and appearance. LLFF exhibits some artifacts on the microphone and some ghosting artifact in the other scenes. SRN produces distorted and blurry rendering for every scene. Neural Volumesstruggle capturing details we can see from the ship reeconstruction}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Comparison on the test set of the real images. As expected LLFF is performing pretty well being projected for this specific use case(forward-facing captures of real scenes). Anyway NeRF is able to represent fine geometry more consistently across rendered views than LLFF as we can see in Fern's and in T-rex. NeRF is also able to reproduce partially occluded scene as in the second row. SRN instead completely fail to represent any high-frequency content.}}{21}{figure.caption.14}\protected@file@percent }
\newlabel{fig:blender1}{{1.7}{21}{Comparison on the test set of the real images. As expected LLFF is performing pretty well being projected for this specific use case(forward-facing captures of real scenes). Anyway NeRF is able to represent fine geometry more consistently across rendered views than LLFF as we can see in Fern's and in T-rex. NeRF is also able to reproduce partially occluded scene as in the second row. SRN instead completely fail to represent any high-frequency content}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Examples of frames with their corresponding manually binary pixelwise mask}}{22}{figure.caption.24}\protected@file@percent }
\newlabel{fig:exMasks}{{1.9}{22}{Examples of frames with their corresponding manually binary pixelwise mask}{figure.caption.24}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Galileo}{23}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:appendix_galileo}{{A}{23}{EPIC-Diff benchmark}{appendix.A}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Math Notation}{24}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:MathNotation}{{B}{24}{EPIC-Diff benchmark}{appendix.B}{}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{25}{appendix.B}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{63D0D05AFC4CA04C8717D9DF9D18891E}
\abx@aux@defaultrefcontext{0}{EPICKITCHENS}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{residualImage}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{fasterRCNN}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{captioning}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{vqa}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{pascalImage}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{imagenet}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{COCO}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ADE20K}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{somethingSomething}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{yt}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{movieBench}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{movieQA}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{vlogs}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{charades}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{EK100}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{visor35}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{visor5}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{neuraldiff}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerfw}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Tnerf}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{MG}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{visor}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ego4d}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{CVPRtutorial}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf15}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf32}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf11}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf27}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf29}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf42}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf21}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf5}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf48}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf2}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf4}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf10}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf23}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf19}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf9}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf13}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf17}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf16}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf26}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nerf35}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{deepvoxels}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{LLFF}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{neuralvol}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{srn}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ndiff_2}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ndiff_18}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ndiff_1}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ndiff_3}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ndiff_38}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ndiff_17}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ndiff_30}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ndiff_15}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ndiff_34}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{colmap}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{examplewebsite}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{sfm_matlab}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{open_cv}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Lowe04_SIFT}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{BAY_SURF}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Brief}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ORB}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bundle}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{schoenberger2016sfm}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{schoenberger2016mvs}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Ranftl2021}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Ranftl2022}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{DIML}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{megadepth}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{redweb}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{wsvd}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{diw}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{eth3d}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{sintel}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kitti}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nyudv2}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{tum_rgbd}{none/global//global/global}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{43}
