\section{Structure from Motion: SfM}\label{sec:sfm}
\textit{Structure from motion (SfM) is the process of estimating the 3D
structure of a scene from a set of 2D images. SfM is used in many
applications, such as 3-D scanning, augmented reality, and visual
simultaneous localization and mapping}~\cite{sfm_matlab}.

We can compute SfM in different ways depending on the data and tools at our disposal.
Factors such as the \textbf{type} and the \textbf{number} of cameras can imply using different methods. Also 
the \textbf{ordering} of the frames may be relevant for a successful reconstruction.

The most basic scenario consists in two images captured from the same camera from two different point of view:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/relatedWorks/MultiView.png} 
    \caption{\textbf{Basic SfM Scenario.} Two cameras capturing the same object from different viewpoints.}
    \label{fig:sfm_basic}
\end{figure}
 In this case the 3D structure can be recovered \textit{up to scale}, meaning that the relations between the obtained
 points is faithful to the reality, but it can differ from the real size by a \textit{scale} factor. The scale factor
 could be retrieved if we know the size of an object in the scene or by having informations from other sensors.

 In the basic scenario the method consists in the following main steps:
 \begin{enumerate}
    \item \textbf{Feature Detection} and \textbf{Matching}: consists in finding relevant points in each image and find the corresponding,if exist, in other images.
    \item Estimating \textbf{Fundamental matrix}: which describes the geometric relationship between two images of a scene taken from different viewpoints. 
    \item Estimating \textbf{Essential Matrix} from Fundamental matrix: which represents the intrinsic geometry of the scene, independent of the camera parameters.
    \item Estimating \textbf{Camera Pose} from Essential Matrix: in this step the positions of the cameras in the world are extracted.
 \end{enumerate}

 \subsection{Feature Detection and Matching }
 In order to find corresponding points in two images we need to detect some points of interest in each image.
 Instead of trying to match each pixel, which would be computational inefficient and possibly misleading due to color,
 it has been shown succesful to use \textit{feature points}. Feature points are relavant points which encapsulate the local
 appeareance of its surrounding pixels which is invariant under changes in illumination, translation, scale and in-plane rotation.
 Good features are \textit{unique}, \textit{can be easily tracked} and \textit{can be easily compared}.

 A practical example could be to imagine how us humans find correspondances in pictures, looking at Figure
~\ref{fig:sfm_opencv}. If we would be asked to find the exact position of the six patches in the underlying image,
the job would be easier for patches E and F, being corners they have less possible misleading correspondances, as happens
instead for patches A or B.
 \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/relatedWorks/feature_building.jpg} 
    \caption{\textbf{Feature Detection Example}~\cite{open_cv}. The easier patches to match to the background are the ones with sharp corners.}
    \label{fig:sfm_opencv}
\end{figure}


In a similar way computers extract features. Two of the first algorithms are in fact based on corners detection: 
\textbf{Harris Corner Detector} and \textbf{Shi-Tomasi Corner Detector}.
Depending on the differences between the two images to be compared, different algorithms have been developed. Here I describe 
some of them as reported in~\cite{open_cv}:
\begin{itemize}
    \item \textbf{SIFT} ~\cite{Lowe04_SIFT}: Harris corner detector is not good enough when scale of image changes. Lowe developed a breakthrough 
    method to find scale-invariant features and it is called SIFT. An example of SIFT features is reported in Figure~\ref{fig:sift}.
    \item \textbf{SURF} (Speeded-Up Robust Features)~\cite{BAY_SURF}: faster SIFT version achieved by using a box filter approximation for the computation of image derivatives
    instead of Gaussian Filters like in SIFT.
    \item \textbf{FAST Algorithm for corner detection} All the above feature detection methods are good in some way. But they 
    are not fast enough to work in real-time applications like SLAM. There comes the FAST algorithm, which is really "FAST".
    \item \textbf{BRIEF}(Binary Robust Independent Elementary Features)~\cite{Brief}:SIFT uses a feature descriptor with 128 floating point numbers.
     Consider thousands of such features. It takes lots of memory and more time for matching. We can compress it to make it
    faster. But still we have to calculate it first. There comes BRIEF which gives the shortcut to find binary descriptors with less memory, faster matching, still higher recognition rate.
    \item \textbf{ORB(Oriented FAST and Rotated BRIEF)}~\cite{ORB}: Open source alternative to SIFT and SURF released by OpenCV.
\end{itemize}
\begin{figure}[t]
    \centerline{
    \includegraphics[width=1.2\linewidth]{images/relatedWorks/Sift_out.png}} % Replace "example-image" with your image file name
    \caption{\textbf{SIFT features position.}Example of interest points, we can see that most of them are placed around corners or edges.}
    \label{fig:sift}
\end{figure}

After having found these points of interest, we need to match them from the different pictures. This step is also known
as \textit{Feature Matching}. The most basics algorithms are:
\begin{itemize}
    \item \textbf{Brute-Force Matcher}: Given a set of images, for each one the descriptor of one feature is compared with th one of every other image using a distance metric.
            The matched feature is the one whose distance is the smallest. It is called Brute-Force because it involves nearly no optimization, taking all possible combinations
            of images. In Figure~\ref{fig:politoCorresponance} is reported the output of a Brute-Force Matcher.
    \item \textbf{FLANN}: which stands for \textit{Fast Library for Approximate Nearest Neighbors}. It contains a collection of algorithms optimized
     for fast nearest neighbor search in large datasets and for high dimensional features, such as hierarchical k-means clustering. It works faster than BFMatcher for large datasets.
     FLANN focuses on giving approximate nearest neighbor search rather than exact matches. Sacrificing some accuracy in favor of speed makes it suitable for large-scale datasets.
     In this way the feature matching problem becomes a clustering problem.

\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/relatedWorks/correspondance.png} % Replace "example-image" with your image file name
    \caption{\textbf{Features correspondence} computed using BruteForce Matcher on a sample image.}
    \label{fig:politoCorresponance}
\end{figure}


 \subsection{Estimating Fundamental and Essential Matrices}
The Fundamental (F) and the Essential (E) matrices allow to relate the projection of a point located in space from one image to the other.
These matrices are based on the so called \textit{Epipolar Geometry}, which describes the relationship between two images.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/relatedWorks/epipolar.png} % Replace "example-image" with your image file name
    \caption{\textbf{Point correspondence geometry.} In particular are reported: the \textit{epipoles} e, e';the \textit{epipolar plane}, which
    is any plane containing the line that connects the two camera centers, also known as \textit{baseline}; the \textit{Epipolar line} which 
    is the intersection of any epipolar plane with the image plane.}
    \label{fig:epipolar}
\end{figure}
As we can see from Figure~\ref{fig:epipolar}, given two cameras $C_l$ and $C_r$ the following definitions can be given:
\begin{itemize}
    \item The \textbf{Epipole}: which is the point of intersection of the \textbf{baseline}(the line that connects the two camera centers
    $C_l$ and $C_r$) with the image plane. In Figure~\ref{fig:epipolar} denoted by $e_i$.
    \item An \textbf{Epipolar plane}, which is any plane containing the baseline.
    \item An \textbf{Epipolar line} which is the intersection of any epipolar plane with the image plane.

\end{itemize}
Now that we have a clear understanding of the underlying geometry, let's proceed to see the actual derivation of the
two matrices.

Let's consider the following scenario,reported in Figure~\ref{fig:EstimateMatrices}: we have a point X and two cameras $C_l$ and $C_r$. 
The relative positions are respectively $x_l$ and $x_r$, while the pixel projections are $p_l$ and $p_r$.
If we consider as reference the left camera, the position of the right one is shifted of a vector \textbf{t}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/relatedWorks/Essential.png} % Replace "example-image" with your image file name
    \caption{A point X viewed from two cameras with $x_l$ being the distance of X from $C_l$ and $x_r$ being the distance of X from $C_r$.
    $p_r$ and $p_l$ are instead the projection of X on the respective camera's focal plane.}
    \label{fig:EstimateMatrices}
\end{figure}

We can extract the Essential Matrix by making the following considerations:
\begin{equation}
    \label{eq:E1}
    x_l \cdot (t \times x_l) = 0
\end{equation}
but $x_l$ can be written as:
\begin{equation}
    x_l = R x_r + t
\end{equation}
So Equation~\ref{eq:E1} becomes:
\begin{equation}
    x_l \cdot (t \times (R x_r + t)) = x_l \cdot (t \times R x_r) \stackrel{a}{=}x_l^T  \left[ t\right]_{\times}  R x_r= 0
\end{equation}
where equality (a) is due to the cross-product matrix notation\footnote{
    \[
  \mathbf{a} \times  \mathbf{b}= \left[\mathbf{a}\right] _{\times} \mathbf{b}= \begin{bmatrix}
    0 & -a_3 & a_2 \\
    a_3 & 0 & -a_1 \\
    -a_2 & a_1     & 0     \\
  \end{bmatrix}
  \begin{bmatrix}
    b_1 \\
    b_2\\
    b_3\\
  \end{bmatrix}
\]}.
We call Essential Matrix the term $ \left[ t\right]_{\times}  R$, such that:
\begin{equation}
    \label{eq:EssentialMatrix}
    x_l^T  \left[ t\right]_{\times}  R x_r=x_l^T E x_r= 0
\end{equation}
In a similar way we can get the Fundamental Matrix, by replacing the relative positions
with the pixel positions. Recalling that the pixel positions are linked to the relative positions
by:
\begin{alignat}{2}
    &p_l = \frac{1}{z_l}K_l x_l  &\quad &p_r = \frac{1}{z_r}K_r x_r
\end{alignat}
where $z_i$ are the focal distances.We can substitute in Eq.~\ref{eq:EssentialMatrix} and obtain:
\begin{equation}
    p_l^T z_l K^{-1^T}_l E K_r^{-1} z_r p_r = 0 \quad z_l,z_r \neq 0
\end{equation}
Since $z_i$ are constants can be simplified, obtaining:
\begin{equation}
    \label{eq:Fund}
    p_l^T  K^{-1^T}_l E K_r^{-1} p_r = p_l^T  F p_r=0 \quad z_l,z_r \neq 0
\end{equation}
where F is the Fundamental Matrix. We can thus write the relation between the two matrices:
\begin{equation}
    E = K_l ^{T} F K_r
\end{equation}



 \subsection{Estimating Camera Pose from Essential Matrix}
 Since  $ \left[ t\right]_{\times} $ is skew symmetric and R is orthonormal (since 
it is a rotation matrix), if we know the Essential Matrix we can decompose it
in its components using singular value decomposition, as we can see from Equations~\ref{eq:t},~\ref{eq:tR},~\ref{eq:W}

%\begin{equation}  \label{eq:t}
%    \left[ t\right]_{\times} =  U W \Sigma U^T
%\end{equation}
%
%\begin{equation} \label{eq:tR}
%    R = U W^T V^T
%\end{equation}
\begin{align}
    \left[ t\right]_{\times} &=  U W \Sigma U^T \label{eq:t}\\
    R &= U W^T V^T  \label{eq:tR}\\
    W&=
    \begin{bmatrix}
      0 & -1& 0 \\
      1 & 0 & 0 \\
      0 & 0 & 1     \\
    \end{bmatrix}\label{eq:W}
\end{align}


\subsection{Multi-view Structure from Motion}
Now that we have grasped the basics to find images correspondances, let's see how we can relate multiple image to recover the structure of a scene.
The last stage is called \textit{bundle adjustment} and it is a iterative algorithm used to adjust structure and motion parameters by minimising a cost function.
The possible methods for bundle adjustment are described in~\cite{bundle}: 
\begin{itemize}
    \item \textbf{Sequential}: it works by considering an additional images at each time, extending in this way the initial reconstruction.
    \item \textbf{Factorization}: it works by computing camera poses and scene geometry using every image measurement at the same time.
\end{itemize}

Bundle Adjustment is needed since the image measurements are usually noisy. It adjusts the positions and orientations of cameras, as well as the 3D coordinates of
points in the scene, to minimize the difference between the observed image features and their predicted locations in the reconstructed 3D model. In simpler terms,
it's like fine-tuning a puzzle to make sure all the pieces fit together perfectly, optimizing the alignment of the cameras and the 3D points to improve the accuracy 
of the reconstruction.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/relatedWorks/MultiViews.png} % Replace "example-image" with your image file name
    \caption{\textbf{Multiple View Scenario.} In multiple view scenario multiple cameras are present, each one recording the scene from a different point of view.}
    \label{fig:multiview}
\end{figure}
\input{content/RelatedWorks/Photogrammetry/colmap.tex}

\input{content/RelatedWorks/Photogrammetry/monocular.tex}