\section{Structure from Motion: SfM}
\textit{Structure from motion (SfM) is the process of estimating the 3-D
structure of a scene from a set of 2-D images. SfM is used in many
applications, such as 3-D scanning, augmented reality, and visual
simultaneous localization and mapping} \cite{sfm_matlab}.

We can compute SfM in different ways depending on the data and tools at our disposal.
Factors such as the \textbf{type} and the \textbf{number} of cameras can imply using different methods. Also 
the \textbf{ordering} of the frames may be relevant for a succesful reconstruction.

The most basic scenario consists in two images captured from the same camera from two different point of view:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/relatedWorks/camera_Sfm.png} % Replace "example-image" with your image file name
    \caption{Basic SfM Scenario}
    \label{fig:sfm_basic}
\end{figure}
 In this case the 3-D structure can be recovered \textit{up to scale}, meaning that the relations between the obtained
 points is faithful to the reality, but it can differ from the real size by a \textit{scale} factor. The scale factor
 could be retrieved if we know the size of an object in the scene or by having informations from other sensors.

 In the basic scenario the method consists in the following main steps:
 \begin{enumerate}
    \item \textbf{Feature Detection} and \textbf{Matching}
    \item Estimating \textbf{Fundamental matrix}
    \item Estimating \textbf{Essential Matrix} from Fundamental matrix
    \item Estimating \textbf{Camera Pose} from Essential Matrix
 \end{enumerate}

 \subsection{Feature Detection and Matching }
 In order to find corresponding points in two images we need to detect some points of interest in each image.
 Instead of trying to match each pixel, which would be computational inefficient and possibly misleading due to color,
 it has been shown succesful to use \textit{feature points}. Feature points are relavant points which encapsulate the local
 appeareance of its surrounding pixels which is invariant under changes in illumination, translation, scale and in-plane rotation.
 Good features are \textit{unique}, \textit{can be easily tracked} and \textit{can be easily compared}.

 A practical example could be to imagine how us humans find correspondances in pictures, looking at image
 \ref{fig:sfm_opencv}. If we would be asked to find the exact position of the six patches in the underlying image,
the job would be easier for patches E and F, being corners they have less possible misleading correspondances, as happens
instead for patches A or B.
 \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/relatedWorks/feature_building.jpg} % Replace "example-image" with your image file name
    \caption{Feature Detection Example \cite{open_cv}}
    \label{fig:sfm_opencv}
\end{figure}

In the same way feature extractor works for computers. Two of the first algorithms are infact based on corners detection: 
\textbf{Harris Corner Detector} and \textbf{Shi-Tomasi Corner Detector}.
Depending on the differences between the two images to be compared, different algorithms have been developed. Here I report 
some of them as reported in \cite{open_cv}:
\begin{itemize}
    \item \textbf{SIFT}: Harris corner detector is not good enough when scale of image changes. Lowe developed a breakthrough 
    method to find scale-invariant features and it is called SIFT
    \item \textbf{SURF} (Speeded-Up Robust Features): faster SIFT version
    \item \textbf{FAST Algorithm for corner detection} All the above feature detection methods are good in some way. But they 
    are not fast enough to work in real-time applications like SLAM. There comes the FAST algorithm, which is really "FAST".
    \item \textbf{BRIEF}(Binary Robust Independent Elementary Features)SIFT uses a feature descriptor with 128 floating point numbers.
     Consider thousands of such features. It takes lots of memory and more time for matching. We can compress it to make it
    faster. But still we have to calculate it first. There comes BRIEF which gives the shortcut to find binary descriptors with less memory, faster matching, still higher recognition rate.
    \item \textbf{ORB(Oriented FAST and Rotated BRIEF)} Open source alternative to SIFT and SURF released by OpenCV.
\end{itemize}
\begin{figure}
    \centerline{
    \includegraphics[width=1.2\linewidth]{images/relatedWorks/Sift_out.png}} % Replace "example-image" with your image file name
    \caption{SIFT features extracted}
    \label{fig:sift}
\end{figure}

After having found these points of interest, we need to match them from the different pictures. This step is also known
as \textit{Feature Matching}. The most basics algorithms are:
\begin{itemize}
    \item \textbf{Brute-Force Matcher}: It takes the descriptor of one feature in first set and is matched with all 
            other features in second set using some distance calculation. And the closest one is returned.
    \item \textbf{FLANN}: which stands for \textit{Fast Library for Approximate Nearest Neighbors}. It contains a collection of algorithms optimized
     for fast nearest neighbor search in large datasets and for high dimensional features. It works faster than BFMatcher for large datasets.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/relatedWorks/correspondance.png} % Replace "example-image" with your image file name
    \caption{Features correspondence computed using BruteForce Matcher.}
    \label{fig:politoCorresponance}
\end{figure}


    \footnote{This is a footnote with additional information.}
 \subsection{Estimating Fundamental and Essential Matrices}
The Fundamental (F) and the Essential (E) matrices allow to relate the projection of a point located in space from one image to the other.
These matrices are based on the so called \textit{Epipolar Geometry}, which describes the relationship between two images.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/relatedWorks/epipolar.png} % Replace "example-image" with your image file name
    \caption{Point correspondence geometry. }
    \label{fig:epipolar}
\end{figure}
As we can see from Figure(\ref{fig:epipolar}), given two cameras $C_l$ and $C_r$ the following definitions can be given:
\begin{itemize}
    \item The \textbf{Epipole}: which is the point of intersection of the \textbf{baseline}(the line that connects the two camera centers
    $C_l$ and $C_r$) with the image plane. In Figure(\ref{fig:epipolar}) denoted by $e_i$.
    \item An \textbf{Epipolar plane}, which is any plane containing the baseline.
    \item An \textbf{Epipolar line} which is the intersection of any epipolar plane with the image plane.

\end{itemize}
Now that we have a clear understanding of the underlying geometry, let's proceed to see the actual derivation of the
two matrices.

Let's consider the following scenario,reported in Figure(\ref{fig:EstimateMatrices}): we have a point X and two cameras $C_l$ and $C_r$. 
The relative positions are respectively $x_l$ and $x_r$, while the pixel projections are $p_l$ and $p_r$.
If we consider as reference the left camera, the position of the right one is shifted of a vector \textbf{t}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/relatedWorks/Essential.png} % Replace "example-image" with your image file name
    \caption{}
    \label{fig:EstimateMatrices}
\end{figure}

We can extract the Essential Matrix by making the following considerations:
\begin{equation}
    \label{eq:E1}
    x_l \cdot (t \times x_l) = 0
\end{equation}
but $x_l$ can be written as:
\begin{equation}
    x_l = R x_r + t
\end{equation}
So Equation(\ref{eq:E1}) becomes:
\begin{equation}
    x_l \cdot (t \times (R x_r + t)) = x_l \cdot (t \times R x_r) \stackrel{a}{=}x_l^T  \left[ t\right]_{\times}  R x_r= 0
\end{equation}
where equality (a) is due to the cross-product matrix notation\footnote{
    \[
  \mathbf{a} \times  \mathbf{b}= \left[\mathbf{a}\right] _{\times} \mathbf{b}= \begin{bmatrix}
    0 & -a_3 & a_2 \\
    a_3 & 0 & -a_1 \\
    -a_2 & a_1     & 0     \\
  \end{bmatrix}
  \begin{bmatrix}
    b_1 \\
    b_2\\
    b_3\\
  \end{bmatrix}
\]}.
We call Essential Matrix the term $ \left[ t\right]_{\times}  R$, such that:
\begin{equation}
    \label{eq:EssentialMatrix}
    x_l^T  \left[ t\right]_{\times}  R x_r=x_l^T E x_r= 0
\end{equation}
In a similar way we can get the Fundamental Matrix, by replacing the relative positions
with the pixel positions. Recalling that the pixel positions are linked to the relative positions
by:
\begin{alignat}{2}
    &p_l = \frac{1}{z_l}K_l x_l  &\quad &p_r = \frac{1}{z_r}K_r x_r
\end{alignat}
where $z_i$ are the focal distances.We can substitute in Eq.(\ref{eq:EssentialMatrix}) and obtain:
\begin{equation}
    p_l^T z_l K^{-1^T}_l E K_r^{-1} z_r p_r = 0 \quad z_l,z_r \neq 0
\end{equation}
Since $z_i$ are constants can be simplified, obtaining:
\begin{equation}
    \label{eq:Fund}
    p_l^T  K^{-1^T}_l E K_r^{-1} p_r = p_l^T  F p_r=0 \quad z_l,z_r \neq 0
\end{equation}
where F is the Fundamental Matrix. We can thus write the relation between the two matrices:
\begin{equation}
    E = K_l ^{T} F K_r
\end{equation}



 \subsection{Estimating Camera Pose from Essential Matrix}
 Since  $ \left[ t\right]_{\times} $ is skew symmetric and R is orthonormal(since 
it is a rotation matrix), if we know the Essential Matrix we can decompose it
in its components\footnote{
    \begin{align}
        \left[ t\right]_{\times} &=  U W \Sigma U^T  \\
        R &= U W^T V^T \\
        W&=
        \begin{bmatrix}
          0 & -1& 0 \\
          1 & 0 & 0 \\
          0 & 0 & 1     \\
        \end{bmatrix}
    \end{align}
}using singular value decomposition.


\subsection{Multi-view Structure from Motion}
Now that we have grasped the basics to find images correspondances, let's see how we can relate multiple image to recover the structure of a scene.
The last stage is called \textit{bundel adjustment} and it is a iterative algorithm used to adjust structure and motion parameters by minimising a cost function.
The possible methods for bundle adjustment are:
\begin{itemize}
    \item \textbf{Sequential}: which work by considering an additional images at each time, extending in this way the initial reconstruction.
    \item \textbf{Factorization}: work by computing camera poses and scene geometry using every image measurement at the same time.
\end{itemize}

Bundle Adjustment is needed since the image measurements are usually noisy. Minimising an appropriate cost function we can obtain a clean model as in a Linear Regression.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/relatedWorks/MultiViews.png} % Replace "example-image" with your image file name
    \caption{Multiple View Scenario. }
    \label{fig:multiview}
\end{figure}
\input{content/RelatedWorks/Photogrammetry/colmap.tex}

\input{content/RelatedWorks/Photogrammetry/monocular.tex}