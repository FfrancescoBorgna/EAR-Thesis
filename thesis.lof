\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Reconstruction of a pizza preparation video. The top and bottom frames give us a glance of the action performed during the video while the central pointcloud highlighht the problems of SfM in dynamic environments like the superimposition of the same object on itself or the reconstruction of objects that are not always present in the scene, e.g. the two pizzas.}}{4}{figure.caption.5}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf {Basic Pipeline.} In the Basic Pipeline a video(represented by the image of a person cooking) is subsampled through EF-Sampling, reconstructed via COLMAP, re-sampled on the reconstructed frames and then fed to NeuralDiff. At this stage the frames are decomposed in actor,foreground and background(as can be seen in the frames reported below NeuralDiff). The Clean reconstruction is obtained by running another COLMAP step on the extracted background frames.}}{5}{figure.caption.8}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf {Monocular Pipeline.} The Monocular pipeline share the first part with the Basic Pipeline. A video is subsampled,reconstructed via COLMAP, subsampled again and fed to Neuraldiff. The difference is that here the dynamic layers are projected in 3D and points closer than a distance th are segmented as dynamic.}}{6}{figure.caption.10}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces \textbf {NeuralDiff Pipeline.}In this pipeline we obtain the three motion layers as in the other methods but actually the segmentation is performed querying the static neural renderer with the positions of the points belonging to the COLMAP reconstruction. Each point is segmented as dynamic if its density is less than a predefined value.}}{7}{figure.caption.12}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces Sampling Steps in our pipelines. The initial video is subsampled by EF-Sampling and the resulting frames are fed to COLMAP. Onve COLMAP reconstructed the scene these frames are again subsampled via Intelligent Sampling and fed to NeuralDiff.}}{8}{figure.caption.16}%
\contentsline {figure}{\numberline {1.6}{\ignorespaces Example of overlapping frames(X and Y). The left and central examples present the same level of overlap even though the image frames are closer together in the central example, because the number of features shared are the same. The right examples instead present a higher level of overlapping due to the bigger number of features.}}{9}{figure.caption.18}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of VISOR active annotations, on the left 'wash a knife' include the static sink as active; on the right 'pour spice' static gas stove is active}}{11}{figure.caption.22}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of Precision-recall curve.We can see how the bottom line model represents the worst a model can perform, e.g. predict every sample as it is coming from the same class,if the dataset is balanced. A better model would \textit {tend} to the upper-right corner, which instead represents the best possible model, a model that have maximum precision and recall.}}{13}{figure.caption.23}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Different COLMAP pcd reconstructions changing number of samples. Each row is the same reconstruction viewed from different viewpoints.From top to bottom the number of frames increase. We can see how the number of frames positively affect the reconstruction.}}{15}{figure.caption.27}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Different COLMAP pcd reconstructions changing resolution. Each row is the same reconstructions viewed from different viewpoints. The first report the reconstruction for a resolution of 456x256 and the bottom one the half resolution. It is clear how the resolution has a beneficial impact on the overall reconstruction.}}{16}{figure.caption.28}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Different scenes where monocular depth estimation was performed. In particular on the right we can find the frame that is instead projected(in red) on the left in the 3D reconstruction of that kitchen. The red frustum(pyramid) represents the camera position and orientation in the space.}}{17}{figure.caption.29}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Dynamic points segmented in scene P01-01 using Monocular Pipeline.}}{18}{figure.caption.30}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Different Segmentation changing the distance threshold. Each point is segmented as dynamic if its distance from a pixel projected in 3D space is less than a threshold Th. The scene is P03-04 and in the left side we can find the static part while in the right side we have the dynamic points. Here we can notice how this method is pretty inaccurate.}}{18}{figure.caption.31}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Visualization of the sampling of scene P01-01 for the three different methods: Intelligent, Uniform and AU using 217 frames in total. The left box is a proposal we gave to visualize how the frames actually spread along the temporal axis, where a line is drawn in correspondence of each sample. The right boxes represent instead histograms with the frequencies of the sample on the entire duration of the video.}}{21}{figure.caption.35}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces \textbf {Qualitative results on P01-01 at 228x128}Visualization of the output of the different models trained on different sampling splits for scene P01-01. The first coloumn represent the real frame while the next ones are respectively: the predicted image,which is the combination of:the static part, the foreground and the actor part.}}{23}{figure.caption.37}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Qualitative results for the static reconstruction of P01-01 scene at 217 frames. In red is highlighted a dynamic plate, while in green a dynamic pan.}}{25}{figure.caption.39}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces Qualitative results for the static reconstruction of P03-04 scene. In red is highlighted a dynamic plate, while in green a dynamic pan.}}{26}{figure.caption.40}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces Comparative of the qualitative results for different samplings of the P01-01 scene.}}{28}{figure.caption.41}%
\contentsline {figure}{\numberline {2.13}{\ignorespaces Experiments performed on 228x128 frames for each scene}}{29}{figure.caption.45}%
\contentsline {figure}{\numberline {2.14}{\ignorespaces Comparison of frequencies for the Intelligent and Uniform sampling with the Object Count for the P01-01 scene changing the total number of sampled frames.}}{30}{figure.caption.47}%
\contentsline {figure}{\numberline {2.15}{\ignorespaces Comparison of frequencies for the Intelligent and Uniform sampling with the Object Count for each scene at a fixed split \~1000 frames.}}{31}{figure.caption.48}%
\contentsline {figure}{\numberline {2.16}{\ignorespaces Results for architecture with foreground+actor= fused}}{32}{figure.caption.50}%
\addvspace {10\p@ }
\addvspace {10\p@ }
