\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Reconstruction of a pizza preparation video. The top and bottom frames give us a glance of the action performed during the video while the central pointcloud highlighht the problems of SfM in dynamic environments like the superimposition of the same object on itself or the reconstruction of objects that are not always present in the scene, e.g. the two pizzas.}}{4}{figure.caption.5}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf {Basic Pipeline.} In the Basic Pipeline a video(represented by the image of a person cooking) is subsampled through EF-Sampling, reconstructed via COLMAP, re-sampled on the reconstructed frames and then fed to NeuralDiff. At this stage the frames are decomposed in actor,foreground and background(as can be seen in the frames reported below NeuralDiff). The Clean reconstruction is obtained by running another COLMAP step on the extracted background frames.}}{5}{figure.caption.8}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf {Monocular Pipeline.} The Monocular pipeline share the first part with the Basic Pipeline. A video is subsampled,reconstructed via COLMAP, subsampled again and fed to Neuraldiff. The difference is that here the dynamic layers are projected in 3D and points closer than a distance th are segmented as dynamic.}}{6}{figure.caption.10}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces \textbf {NeuralDiff Pipeline.}In this pipeline we obtain the three motion layers as in the other methods but actually the segmentation is performed querying the static neural renderer with the positions of the points belonging to the COLMAP reconstruction. Each point is segmented as dynamic if its density is less than a predefined value.}}{7}{figure.caption.12}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces Sampling Steps in our pipelines. The initial video is subsampled by EF-Sampling and the resulting frames are fed to COLMAP. Onve COLMAP reconstructed the scene these frames are again subsampled via Intelligent Sampling and fed to NeuralDiff.}}{8}{figure.caption.14}%
\contentsline {figure}{\numberline {1.6}{\ignorespaces Example of overlapping frames(X and Y). The left and central examples present the same level of overlap even though the image frames are closer together in the central example, because the number of features shared are the same. The right examples instead present a higher level of overlapping due to the bigger number of features.}}{9}{figure.caption.15}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of VISOR active annotations, on the left 'wash a knife' include the static sink as active; on the right 'pour spice' static gas stove is active}}{11}{figure.caption.17}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Varying COLMAP pcd reconstruction changing number of samples and resolution. Each row is the same reconstruction viewed from different viewpoints.From top to bottom the number of frames increase, while in the first two rows the same split is compared using different resolutions.}}{13}{figure.caption.19}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Frame and its projection in the 3D space.}}{14}{figure.caption.20}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Scene cleaned from dynamic points. (Dovrei rifarla...)}}{14}{figure.caption.21}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualization of the output of the different models trained on differen splits.}}{17}{figure.caption.23}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Qualitative results for the static reconstruction of P01-01 scene at 217 frames. In red is highlighted a dynamic plate, while in green a dynamic pan.}}{18}{figure.caption.24}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Comparative of the qualitative results for different samplings of the P01-01 scene.}}{21}{figure.caption.25}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Experiments performed on 228x128 frames for each scene}}{22}{figure.caption.27}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Visualization of the sampling for the three different methods: Intelligent, Uniform and AU using 217 frames in total.}}{23}{figure.caption.28}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Comparison of frequencies for the Intelligent and Uniform sampling with the Object Count for the P01-01 scene changing the total number of sampled frames.}}{24}{figure.caption.29}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces Comparison of frequencies for the Intelligent and Uniform sampling with the Object Count for each scene at a fixed split \~1000 frames.}}{25}{figure.caption.30}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces Results for architecture with foreground+actor= fused}}{26}{figure.caption.32}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
