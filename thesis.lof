\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf {NeRF.}Optimization of a continous 5D neural radiance field representation(volume density and view-dependent color at any continuous location) of a scene from a seet of input images. The 2D novel views are obtained thanks to classic volume rendering techniques. Here in this example, given 100 images acquired from different viewpoints, they sample two novel views.}}{4}{figure.caption.4}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf {$F_{\Theta }$ Scheme.} The input position \textbf {x} pass through 8 Fully connected (FC) layers of 256-channels. Each FC layer is followed by a ReLU activation function. This intermediate result is then concatenated with the input direction (\textbf {d}) and fed to one last FC with 128 channels that feeds its output to a ReLU function. The output of the ReLU are the color \textbf {c} and the volume density ($\sigma $).}}{6}{figure.caption.5}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Here are reported the results obtained with different strategies, as written underneath each image. In particular removing view dependence prevents the model from recreating the specular reflection on the bulldozer tread. Removing the Positional encoding instead we obtain a blurred image, meaning that high frequencies are not captured nor represented.}}{6}{figure.caption.6}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces Example of rays passing through an image plane of size 3x3 pixels.}}{7}{figure.caption.8}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces PDF of normalized coarse weights $\hat {w}_i$ along a ray with $N_c$ samples.}}{9}{figure.caption.10}%
\contentsline {figure}{\numberline {1.8}{\ignorespaces Given an egocentric video with camera reconstruction, NeuralDiff, a neural architecture, learns how to decompose each frame into a static background and a a dynamic foreground, which includes every object that sooner or later will move and the actor's body parts. Each of these streams is learned exploiting the characteristics of the scene that is going to be captured. Being a neural radiance field, NeuralDiff is also capable to render images from novel viewpoints as can be seen in the bottom right part of the scene.}}{13}{figure.caption.16}%
\contentsline {figure}{\numberline {1.6}{\ignorespaces Comparison on test images from the newly introduced synthetic dataset. NeRF method is able to recover fine details in both geometry and appearance. LLFF exhibits some artifacts on the microphone and some ghosting artifact in the other scenes. SRN produces distorted and blurry rendering for every scene. Neural Volumesstruggle capturing details we can see from the ship reeconstruction.}}{20}{figure.caption.13}%
\contentsline {figure}{\numberline {1.7}{\ignorespaces Comparison on the test set of the real images. As expected LLFF is performing pretty well being projected for this specific use case(forward-facing captures of real scenes). Anyway NeRF is able to represent fine geometry more consistently across rendered views than LLFF as we can see in Fern's and in T-rex. NeRF is also able to reproduce partially occluded scene as in the second row. SRN instead completely fail to represent any high-frequency content.}}{21}{figure.caption.14}%
\contentsline {figure}{\numberline {1.9}{\ignorespaces Examples of frames with their corresponding manually binary pixelwise mask}}{22}{figure.caption.24}%
\addvspace {10\p@ }
\addvspace {10\p@ }
